{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 13465111,
          "sourceType": "datasetVersion",
          "datasetId": 8547335
        }
      ],
      "dockerImageVersionId": 31154,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.1.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "# Verify\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-03T10:40:41.203485Z",
          "iopub.execute_input": "2025-11-03T10:40:41.203733Z",
          "iopub.status.idle": "2025-11-03T10:42:50.740144Z",
          "shell.execute_reply.started": "2025-11-03T10:40:41.203713Z",
          "shell.execute_reply": "2025-11-03T10:42:50.739501Z"
        },
        "id": "nuOY4trAVx-U",
        "outputId": "984010f7-f4e7-45ed-f6bb-6c476f3ec457"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Looking in indexes: https://download.pytorch.org/whl/cu118\nCollecting torch==2.1.0\n  Downloading https://download.pytorch.org/whl/cu118/torch-2.1.0%2Bcu118-cp311-cp311-linux_x86_64.whl (2325.9 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m724.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:02\u001b[0mm\n\u001b[?25hCollecting torchaudio==2.1.0\n  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.1.0%2Bcu118-cp311-cp311-linux_x86_64.whl (3.2 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (3.19.1)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (4.15.0)\nRequirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (1.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (2025.9.0)\nCollecting triton==2.1.0 (from torch==2.1.0)\n  Downloading https://download.pytorch.org/whl/triton-2.1.0-0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.1.0) (3.0.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.1.0) (1.3.0)\nInstalling collected packages: triton, torch, torchaudio\n  Attempting uninstall: triton\n    Found existing installation: triton 3.2.0\n    Uninstalling triton-3.2.0:\n      Successfully uninstalled triton-3.2.0\n  Attempting uninstall: torch\n    Found existing installation: torch 2.6.0+cu124\n    Uninstalling torch-2.6.0+cu124:\n      Successfully uninstalled torch-2.6.0+cu124\n  Attempting uninstall: torchaudio\n    Found existing installation: torchaudio 2.6.0+cu124\n    Uninstalling torchaudio-2.6.0+cu124:\n      Successfully uninstalled torchaudio-2.6.0+cu124\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.1.0+cu118 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed torch-2.1.0+cu118 torchaudio-2.1.0+cu118 triton-2.1.0\nCUDA available: True\nPyTorch version: 2.1.0+cu118\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# âš ï¸ CRITICAL: Set these BEFORE any TTS imports\n",
        "os.environ['TRANSFORMERS_NO_TORCHAO_IMPORT'] = '1'\n",
        "os.environ['TORCH_ALLOW_UNSAFE_DESERIALIZATION'] = '1'\n",
        "\n",
        "print(\"âœ… Environment variables set\")\n",
        "print(f\"TRANSFORMERS_NO_TORCHAO_IMPORT = {os.environ['TRANSFORMERS_NO_TORCHAO_IMPORT']}\")\n",
        "print(f\"TORCH_ALLOW_UNSAFE_DESERIALIZATION = {os.environ['TORCH_ALLOW_UNSAFE_DESERIALIZATION']}\")\n",
        "\n",
        "# Check Python version\n",
        "print(f\"\\nPython version: {sys.version}\")\n",
        "print(f\"Working directory: {os.getcwd()}\")\n",
        "\n",
        "# Enable GPU\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "# Check GPU\n",
        "import torch\n",
        "print(f\"\\nCUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-03T10:42:50.741556Z",
          "iopub.execute_input": "2025-11-03T10:42:50.741897Z",
          "iopub.status.idle": "2025-11-03T10:42:50.769713Z",
          "shell.execute_reply.started": "2025-11-03T10:42:50.741880Z",
          "shell.execute_reply": "2025-11-03T10:42:50.769141Z"
        },
        "id": "sjq3r1rzVx-Y",
        "outputId": "7f7da72d-8b24-47d0-e6e0-0040f05c7bb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "âœ… Environment variables set\nTRANSFORMERS_NO_TORCHAO_IMPORT = 1\nTORCH_ALLOW_UNSAFE_DESERIALIZATION = 1\n\nPython version: 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\nWorking directory: /kaggle/working\n\nCUDA available: True\nGPU: Tesla P100-PCIE-16GB\nGPU Memory: 17.1 GB\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Install TTS and related packages\n",
        "!pip install -q TTS==0.22.0\n",
        "\n",
        "# âš ï¸ CRITICAL FIX: Use transformers 4.36.0 instead of 4.45.2\n",
        "!pip install -q transformers==4.36.0 tokenizers==0.15.0\n",
        "\n",
        "!pip install -q librosa==0.10.2 soundfile==0.12.1 scipy==1.11.2 pysbd==0.3.4\n",
        "!pip install -q pandas==1.5.3 scikit-learn==1.3.2 tqdm==4.66.3\n",
        "!pip install -q einops==0.7.0 unidecode==1.3.8 inflect==7.0.0\n",
        "!pip install -q coqpit==0.0.16 trainer==0.0.36 mutagen\n",
        "!pip install -q pypinyin hangul_romanize num2words kagglehub\n",
        "\n",
        "print(\"âœ… All dependencies installed successfully!\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-03T10:42:50.770656Z",
          "iopub.execute_input": "2025-11-03T10:42:50.771049Z",
          "iopub.status.idle": "2025-11-03T10:44:45.974416Z",
          "shell.execute_reply.started": "2025-11-03T10:42:50.771031Z",
          "shell.execute_reply": "2025-11-03T10:44:45.973578Z"
        },
        "id": "TGgVz4y0Vx-a",
        "outputId": "92223ce1-1ee1-461e-fb33-5e098f2d6871"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m938.0/938.0 kB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m345.1/345.1 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m163.5/163.5 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m108.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m110.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.2/51.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m840.2/840.2 kB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m293.8/293.8 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.2/72.2 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m104.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25h  Building wheel for gruut (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for encodec (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for bnnumerizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for gruut-ipa (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for gruut_lang_de (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for gruut_lang_en (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for gruut_lang_es (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for gruut_lang_fr (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\ndask-expr 1.1.21 requires pandas>=2, but you have pandas 1.5.3 which is incompatible.\ncudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nwoodwork 0.31.0 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\nfeaturetools 1.31.0 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\nvisions 0.8.1 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\npyldavis 3.4.1 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.7.2 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.5.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nsklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.2 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\nscikit-image 0.25.2 requires networkx>=3.0, but you have networkx 2.8.8 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ntorchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.1.0+cu118 which is incompatible.\nxarray 2025.7.1 requires pandas>=2.2, but you have pandas 1.5.3 which is incompatible.\nmizani 0.13.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\nnx-cugraph-cu12 25.6.0 requires networkx>=3.2, but you have networkx 2.8.8 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\nplotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.36.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.1/59.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m260.0/260.0 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m36.3/36.3 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nwoodwork 0.31.0 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\nfeaturetools 1.31.0 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\npyldavis 3.4.1 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.7.2 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.11.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nsklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.2 which is incompatible.\nsentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.36.0 which is incompatible.\nscikit-image 0.25.2 requires networkx>=3.0, but you have networkx 2.8.8 which is incompatible.\nscikit-image 0.25.2 requires scipy>=1.11.4, but you have scipy 1.11.2 which is incompatible.\nmizani 0.13.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\nplotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nwoodwork 0.31.0 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\nfeaturetools 1.31.0 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\npyldavis 3.4.1 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.11.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nsentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.36.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\numap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.3.2 which is incompatible.\ndataproc-spark-connect 0.8.3 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\ndataproc-spark-connect 0.8.3 requires tqdm>=4.67, but you have tqdm 4.66.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hâœ… All dependencies installed successfully!\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify critical packages\n",
        "import trainer\n",
        "import TTS\n",
        "import transformers\n",
        "import librosa\n",
        "\n",
        "print(f\"trainer version: {trainer.__version__}\")\n",
        "print(f\"TTS installed: {TTS.__version__}\")\n",
        "print(f\"transformers version: {transformers.__version__}\")\n",
        "print(f\"librosa version: {librosa.__version__}\")\n",
        "print(\"âœ… All packages verified!\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-03T10:44:45.976264Z",
          "iopub.execute_input": "2025-11-03T10:44:45.976598Z",
          "iopub.status.idle": "2025-11-03T10:44:48.188449Z",
          "shell.execute_reply.started": "2025-11-03T10:44:45.976570Z",
          "shell.execute_reply": "2025-11-03T10:44:48.187680Z"
        },
        "id": "-N-AQB7iVx-b",
        "outputId": "6e6f8a2c-4f71-454d-f6b1-43ba19233241"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "trainer version: v0.0.36\nTTS installed: 0.22.0\ntransformers version: 4.36.0\nlibrosa version: 0.10.2\nâœ… All packages verified!\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "repo_url = \"https://github.com/amalshafernando/XTTSv2-Finetuning-for-New-Languages.git\"\n",
        "repo_name = \"XTTSv2-Finetuning-for-New-Languages\"\n",
        "\n",
        "# Clone only if it doesn't exist\n",
        "if not os.path.exists(repo_name):\n",
        "    print(f\"ğŸ”¹ Cloning {repo_name}...\")\n",
        "    !git clone {repo_url}\n",
        "    print(\"âœ… Repository cloned\")\n",
        "else:\n",
        "    print(f\"âœ… Repository already exists: {repo_name}\")\n",
        "\n",
        "# Change to repo directory\n",
        "os.chdir(repo_name)\n",
        "print(f\"âœ… Current directory: {os.getcwd()}\")\n",
        "\n",
        "# List contents\n",
        "print(\"\\nğŸ”¹ Repository contents:\")\n",
        "!ls -la\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-03T10:44:48.189052Z",
          "iopub.execute_input": "2025-11-03T10:44:48.189447Z",
          "iopub.status.idle": "2025-11-03T10:44:49.344442Z",
          "shell.execute_reply.started": "2025-11-03T10:44:48.189419Z",
          "shell.execute_reply": "2025-11-03T10:44:49.343568Z"
        },
        "id": "lsw7kG_yVx-c",
        "outputId": "028a12a7-587b-43b1-e63c-10b3aacc9347"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "ğŸ”¹ Cloning XTTSv2-Finetuning-for-New-Languages...\nCloning into 'XTTSv2-Finetuning-for-New-Languages'...\nremote: Enumerating objects: 667, done.\u001b[K\nremote: Counting objects: 100% (356/356), done.\u001b[K\nremote: Compressing objects: 100% (249/249), done.\u001b[K\nremote: Total 667 (delta 146), reused 139 (delta 106), pack-reused 311 (from 2)\u001b[K\nReceiving objects: 100% (667/667), 2.16 MiB | 8.42 MiB/s, done.\nResolving deltas: 100% (170/170), done.\nâœ… Repository cloned\nâœ… Current directory: /kaggle/working/XTTSv2-Finetuning-for-New-Languages\n\nğŸ”¹ Repository contents:\ntotal 124\ndrwxr-xr-x  6 root root 4096 Nov  3 10:44  .\ndrwxr-xr-x  4 root root 4096 Nov  3 10:44  ..\n-rw-r--r--  1 root root    0 Nov  3 10:44 '=0.0.16'\n-rw-r--r--  1 root root 2428 Nov  3 10:44  actual-path.py\n-rw-r--r--  1 root root 3307 Nov  3 10:44  create_sinhala_vocab.py\n-rw-r--r--  1 root root 2461 Nov  3 10:44  download_checkpoint.py\n-rw-r--r--  1 root root 1905 Nov  3 10:44  download_model_manual.py\n-rw-r--r--  1 root root 4469 Nov  3 10:44  extend_vocab_config.py\n-rw-r--r--  1 root root 4060 Nov  3 10:44  extend_vocab_config.py.txt\ndrwxr-xr-x  8 root root 4096 Nov  3 10:44  .git\n-rw-r--r--  1 root root  141 Nov  3 10:44  .gitignore\n-rw-r--r--  1 root root 2821 Nov  3 10:44  kaggle_dataset_setup.py\n-rw-r--r--  1 root root 5320 Nov  3 10:44  kaggle_training.py\n-rw-r--r--  1 root root 1125 Nov  3 10:44  Readme2.md\n-rw-r--r--  1 root root 3199 Nov  3 10:44  README-3.md\n-rw-r--r--  1 root root 5572 Nov  3 10:44  Readme.md\ndrwxr-xr-x  9 root root 4096 Nov  3 10:44  recipes\n-rw-r--r--  1 root root  662 Nov  3 10:44  requirements.txt\n-rw-r--r--  1 root root 1038 Nov  3 10:44  requirements.txt.example\n-rw-r--r--  1 root root 1178 Nov  3 10:44  setup_kaggle.py\n-rw-r--r--  1 root root 7456 Nov  3 10:44  train_dvae_xtts.py\n-rwxr-xr-x  1 root root  235 Nov  3 10:44  train_dvae_xtts.sh\n-rw-r--r--  1 root root 9483 Nov  3 10:44  train_gpt_xtts.py\n-rwxr-xr-x  1 root root  381 Nov  3 10:44  train_gpt_xtts.sh\ndrwxr-xr-x 11 root root 4096 Nov  3 10:44  TTS\ndrwxr-xr-x  2 root root 4096 Nov  3 10:44  XTTS-v2\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import shutil\n",
        "import kagglehub\n",
        "import os\n",
        "\n",
        "# Download dataset\n",
        "path = kagglehub.dataset_download(\"amalshaf/sinhala-tts-dataset\")\n",
        "print(f\"Dataset downloaded to: {path}\")\n",
        "\n",
        "# Setup paths\n",
        "kaggle_dataset_path = f\"{path}/sinhala-tts-dataset\"\n",
        "print(f\"kaggle dataset path: {kaggle_dataset_path}\")\n",
        "target_dataset_path = \"/kaggle/working/datasets/\"\n",
        "print(f\"target dataset path: {target_dataset_path}\")\n",
        "\n",
        "\n",
        "# Create target directory\n",
        "os.makedirs(f\"{target_dataset_path}/wavs\", exist_ok=True)\n",
        "\n",
        "# Copy audio files\n",
        "if os.path.exists(f\"{kaggle_dataset_path}/wavs\"):\n",
        "    shutil.copytree(f\"{kaggle_dataset_path}/wavs\", f\"{target_dataset_path}/wavs\", dirs_exist_ok=True)\n",
        "    print(f\"âœ… Copied audio files\")\n",
        "\n",
        "# Convert CSV to XTTS format\n",
        "df_train = pd.read_csv(f\"{kaggle_dataset_path}/metadata_train.csv\", sep='|')\n",
        "df_train_xtts = pd.DataFrame()\n",
        "df_train_xtts['audio_file'] = df_train['audio_file_path'].apply(lambda x: x.replace('wav/', 'wavs/'))\n",
        "df_train_xtts['text'] = df_train['transcript']\n",
        "df_train_xtts['speaker_name'] = df_train['speaker_id']\n",
        "df_train_xtts.to_csv(f\"{target_dataset_path}/metadata_train.csv\", sep='|', index=False)\n",
        "\n",
        "df_eval = pd.read_csv(f\"{kaggle_dataset_path}/metadata_eval.csv\", sep='|')\n",
        "df_eval_xtts = pd.DataFrame()\n",
        "df_eval_xtts['audio_file'] = df_eval['audio_file_path'].apply(lambda x: x.replace('wav/', 'wavs/'))\n",
        "df_eval_xtts['text'] = df_eval['transcript']\n",
        "df_eval_xtts['speaker_name'] = df_eval['speaker_id']\n",
        "df_eval_xtts.to_csv(f\"{target_dataset_path}/metadata_eval.csv\", sep='|', index=False)\n",
        "\n",
        "print(f\"âœ… Training samples: {len(df_train_xtts)}\")\n",
        "print(f\"âœ… Validation samples: {len(df_eval_xtts)}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-03T10:44:49.345555Z",
          "iopub.execute_input": "2025-11-03T10:44:49.346319Z",
          "iopub.status.idle": "2025-11-03T10:44:59.020730Z",
          "shell.execute_reply.started": "2025-11-03T10:44:49.346295Z",
          "shell.execute_reply": "2025-11-03T10:44:59.020053Z"
        },
        "id": "DI7TjKcXVx-c",
        "outputId": "ba891287-bf9d-4bf8-eece-6140f8104f01"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Dataset downloaded to: /kaggle/input/sinhala-tts-dataset\nkaggle dataset path: /kaggle/input/sinhala-tts-dataset/sinhala-tts-dataset\ntarget dataset path: /kaggle/working/datasets/\nâœ… Copied audio files\nâœ… Training samples: 1000\nâœ… Validation samples: 251\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Run download script\n",
        "#!python download_checkpoint.py --output_path checkpoints/\n",
        "# Download XTTS-v2 base model\n",
        "#!python download_checkpoint.py --output_path /kaggle/working/checkpoints/\n",
        "\n",
        "#print(\"âœ… XTTS-v2 model downloaded\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-03T10:44:59.021570Z",
          "iopub.execute_input": "2025-11-03T10:44:59.021786Z",
          "iopub.status.idle": "2025-11-03T10:44:59.025532Z",
          "shell.execute_reply.started": "2025-11-03T10:44:59.021770Z",
          "shell.execute_reply": "2025-11-03T10:44:59.024760Z"
        },
        "id": "wbkaUyP0Vx-d"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 8 (CORRECTED): Download XTTS-v2 Model Files\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Create output directory\n",
        "output_dir = \"/kaggle/working/checkpoints/XTTS_v2.0_original_model_files\"\n",
        "#output_dir = \"/kaggle/working/checkpoints/XTTS_v2\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"DOWNLOADING XTTS-v2 MODEL FILES\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Define all required files from Hugging Face\n",
        "base_url = \"https://huggingface.co/coqui/XTTS-v2/resolve/main/\"\n",
        "\n",
        "files_to_download = {\n",
        "    \"config.json\": f\"{base_url}config.json\",\n",
        "    \"vocab.json\": f\"{base_url}vocab.json\",\n",
        "    \"model.pth\": f\"{base_url}model.pth\",\n",
        "    \"dvae.pth\": f\"{base_url}dvae.pth\",\n",
        "    \"mel_stats.pth\": f\"{base_url}mel_stats.pth\",\n",
        "    \"speakers_xtts.pth\": f\"{base_url}speakers_xtts.pth\",\n",
        "}\n",
        "\n",
        "def download_file(url, output_path):\n",
        "    '''Download file with progress bar'''\n",
        "    response = requests.get(url, stream=True)\n",
        "    total_size = int(response.headers.get('content-length', 0))\n",
        "\n",
        "    with open(output_path, 'wb') as f:\n",
        "        with tqdm(total=total_size, unit='B', unit_scale=True, desc=os.path.basename(output_path)) as pbar:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                if chunk:\n",
        "                    f.write(chunk)\n",
        "                    pbar.update(len(chunk))\n",
        "\n",
        "# Download each file\n",
        "for filename, url in files_to_download.items():\n",
        "    output_path = os.path.join(output_dir, filename)\n",
        "\n",
        "    if os.path.exists(output_path):\n",
        "        print(f\"âœ… {filename} already exists, skipping...\")\n",
        "    else:\n",
        "        print(f\"\\nğŸ”¹ Downloading {filename}...\")\n",
        "        try:\n",
        "            download_file(url, output_path)\n",
        "            print(f\"âœ… {filename} downloaded successfully\")\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Failed to download {filename}: {e}\")\n",
        "\n",
        "# Verify all files downloaded\n",
        "print(f\"\\n{'=' * 80}\")\n",
        "print(\"VERIFICATION\")\n",
        "print(f\"{'=' * 80}\")\n",
        "\n",
        "all_downloaded = True\n",
        "for filename in files_to_download.keys():\n",
        "    filepath = os.path.join(output_dir, filename)\n",
        "    if os.path.exists(filepath):\n",
        "        size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
        "        print(f\"âœ… {filename}: {size_mb:.1f} MB\")\n",
        "    else:\n",
        "        print(f\"âŒ {filename}: MISSING!\")\n",
        "        all_downloaded = False\n",
        "\n",
        "if all_downloaded:\n",
        "    print(f\"\\n{'=' * 80}\")\n",
        "    print(\"âœ… ALL XTTS-v2 MODEL FILES DOWNLOADED SUCCESSFULLY!\")\n",
        "    print(f\"{'=' * 80}\")\n",
        "\n",
        "# List all downloaded files\n",
        "print(f\"\\nğŸ”¹ Contents of {output_dir}:\")\n",
        "!ls -lh {output_dir}\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-03T10:44:59.026377Z",
          "iopub.execute_input": "2025-11-03T10:44:59.026634Z",
          "iopub.status.idle": "2025-11-03T10:45:08.753183Z",
          "shell.execute_reply.started": "2025-11-03T10:44:59.026613Z",
          "shell.execute_reply": "2025-11-03T10:45:08.752198Z"
        },
        "id": "qWtTrnHzVx-d",
        "outputId": "e5528965-e4a2-4903-deeb-6229774d389d"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "================================================================================\nDOWNLOADING XTTS-v2 MODEL FILES\n================================================================================\n\nğŸ”¹ Downloading config.json...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "config.json: 4.37kB [00:00, 6.64MB/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "âœ… config.json downloaded successfully\n\nğŸ”¹ Downloading vocab.json...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\nvocab.json: 361kB [00:00, 29.3MB/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "âœ… vocab.json downloaded successfully\n\nğŸ”¹ Downloading model.pth...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "model.pth: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.87G/1.87G [00:07<00:00, 266MB/s] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "âœ… model.pth downloaded successfully\n\nğŸ”¹ Downloading dvae.pth...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "dvae.pth: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 211M/211M [00:00<00:00, 279MB/s] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "âœ… dvae.pth downloaded successfully\n\nğŸ”¹ Downloading mel_stats.pth...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "mel_stats.pth: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.07k/1.07k [00:00<00:00, 8.93MB/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "âœ… mel_stats.pth downloaded successfully\n\nğŸ”¹ Downloading speakers_xtts.pth...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "speakers_xtts.pth: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.75M/7.75M [00:00<00:00, 90.9MB/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "âœ… speakers_xtts.pth downloaded successfully\n\n================================================================================\nVERIFICATION\n================================================================================\nâœ… config.json: 0.0 MB\nâœ… vocab.json: 0.3 MB\nâœ… model.pth: 1781.4 MB\nâœ… dvae.pth: 200.8 MB\nâœ… mel_stats.pth: 0.0 MB\nâœ… speakers_xtts.pth: 7.4 MB\n\n================================================================================\nâœ… ALL XTTS-v2 MODEL FILES DOWNLOADED SUCCESSFULLY!\n================================================================================\n\nğŸ”¹ Contents of /kaggle/working/checkpoints/XTTS_v2.0_original_model_files:\ntotal 2.0G\n-rw-r--r-- 1 root root 4.3K Nov  3 10:44 config.json\n-rw-r--r-- 1 root root 201M Nov  3 10:45 dvae.pth\n-rw-r--r-- 1 root root 1.1K Nov  3 10:45 mel_stats.pth\n-rw-r--r-- 1 root root 1.8G Nov  3 10:45 model.pth\n-rw-r--r-- 1 root root 7.4M Nov  3 10:45 speakers_xtts.pth\n-rw-r--r-- 1 root root 353K Nov  3 10:44 vocab.json\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#cell 9\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# Verify vocab.json location\n",
        "vocab_path = \"/kaggle/working/checkpoints/XTTS_v2.0_original_model_files/vocab.json\"\n",
        "#vocab_path = \"/kaggle/working/checkpoints/XTTS_v2/vocab.json\"\n",
        "print(f\"Checking for vocab.json at: {vocab_path}\")\n",
        "\n",
        "if os.path.exists(vocab_path):\n",
        "    print(f\"âœ… Found vocab.json at: {vocab_path}\")\n",
        "else:\n",
        "    print(f\"âŒ vocab.json not found at: {vocab_path}\")\n",
        "    print(\"\\nSearching for vocab.json in checkpoints directory...\")\n",
        "    vocab_files = glob.glob(\"checkpoints/**/vocab.json\", recursive=True)\n",
        "    if vocab_files:\n",
        "        print(f\"Found vocab files at: {vocab_files}\")\n",
        "        vocab_path = vocab_files[0]\n",
        "    else:\n",
        "        print(\"âŒ No vocab.json found! Download may have failed.\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-03T10:45:08.754527Z",
          "iopub.execute_input": "2025-11-03T10:45:08.755107Z",
          "iopub.status.idle": "2025-11-03T10:45:08.761185Z",
          "shell.execute_reply.started": "2025-11-03T10:45:08.755080Z",
          "shell.execute_reply": "2025-11-03T10:45:08.760395Z"
        },
        "id": "ROYEdapvVx-e",
        "outputId": "63235a81-c8f4-430f-ffa2-c215147f93f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Checking for vocab.json at: /kaggle/working/checkpoints/XTTS_v2.0_original_model_files/vocab.json\nâœ… Found vocab.json at: /kaggle/working/checkpoints/XTTS_v2.0_original_model_files/vocab.json\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 9 (CORRECTED): Create XTTS-v2 Directory Structure\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"SETTING UP XTTS-v2 DIRECTORY STRUCTURE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Source: where files were downloaded\n",
        "source_dir = \"/kaggle/working/checkpoints/XTTS_v2.0_original_model_files\"\n",
        "\n",
        "# Target: where extend_vocab_config.py expects them\n",
        "target_dir = \"/kaggle/working/checkpoints/XTTS-v2\"\n",
        "\n",
        "# Create target directory if it doesn't exist\n",
        "if not os.path.exists(target_dir):\n",
        "    os.makedirs(target_dir, exist_ok=True)\n",
        "    print(f\"âœ… Created directory: {target_dir}\")\n",
        "\n",
        "# Copy all files from source to target\n",
        "files_to_copy = [\"config.json\", \"vocab.json\", \"model.pth\", \"dvae.pth\", \"mel_stats.pth\", \"speakers_xtts.pth\"]\n",
        "\n",
        "for filename in files_to_copy:\n",
        "    source_path = os.path.join(source_dir, filename)\n",
        "    target_path = os.path.join(target_dir, filename)\n",
        "\n",
        "    if os.path.exists(source_path):\n",
        "        if not os.path.exists(target_path):\n",
        "            shutil.copy2(source_path, target_path)\n",
        "            print(f\"âœ… Copied {filename}\")\n",
        "        else:\n",
        "            print(f\"âœ… {filename} already exists in target\")\n",
        "    else:\n",
        "        print(f\"âŒ Source file missing: {filename}\")\n",
        "\n",
        "# Verify all files are in place\n",
        "print(f\"\\n{'=' * 80}\")\n",
        "print(\"VERIFICATION\")\n",
        "print(f\"{'=' * 80}\")\n",
        "\n",
        "all_present = True\n",
        "for filename in files_to_copy:\n",
        "    filepath = os.path.join(target_dir, filename)\n",
        "    if os.path.exists(filepath):\n",
        "        size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
        "        print(f\"âœ… {filename}: {size_mb:.1f} MB\")\n",
        "    else:\n",
        "        print(f\"âŒ {filename}: MISSING!\")\n",
        "        all_present = False\n",
        "\n",
        "if all_present:\n",
        "    print(f\"\\n{'=' * 80}\")\n",
        "    print(\"âœ… XTTS-v2 DIRECTORY STRUCTURE READY!\")\n",
        "    print(f\"{'=' * 80}\")\n",
        "else:\n",
        "    print(f\"\\n{'=' * 80}\")\n",
        "    print(\"âŒ SOME FILES ARE MISSING!\")\n",
        "    print(f\"{'=' * 80}\")\n",
        "\n",
        "# List final structure\n",
        "print(f\"\\nğŸ”¹ Contents of {target_dir}:\")\n",
        "!ls -lh {target_dir}\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-03T10:45:08.763977Z",
          "iopub.execute_input": "2025-11-03T10:45:08.764198Z",
          "iopub.status.idle": "2025-11-03T10:45:10.647890Z",
          "shell.execute_reply.started": "2025-11-03T10:45:08.764157Z",
          "shell.execute_reply": "2025-11-03T10:45:10.646668Z"
        },
        "id": "GIDoiU88Vx-f",
        "outputId": "2c343150-31f8-4858-bb69-600280983fe3"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "================================================================================\nSETTING UP XTTS-v2 DIRECTORY STRUCTURE\n================================================================================\nâœ… Created directory: /kaggle/working/checkpoints/XTTS-v2\nâœ… Copied config.json\nâœ… Copied vocab.json\nâœ… Copied model.pth\nâœ… Copied dvae.pth\nâœ… Copied mel_stats.pth\nâœ… Copied speakers_xtts.pth\n\n================================================================================\nVERIFICATION\n================================================================================\nâœ… config.json: 0.0 MB\nâœ… vocab.json: 0.3 MB\nâœ… model.pth: 1781.4 MB\nâœ… dvae.pth: 200.8 MB\nâœ… mel_stats.pth: 0.0 MB\nâœ… speakers_xtts.pth: 7.4 MB\n\n================================================================================\nâœ… XTTS-v2 DIRECTORY STRUCTURE READY!\n================================================================================\n\nğŸ”¹ Contents of /kaggle/working/checkpoints/XTTS-v2:\ntotal 2.0G\n-rw-r--r-- 1 root root 4.3K Nov  3 10:44 config.json\n-rw-r--r-- 1 root root 201M Nov  3 10:45 dvae.pth\n-rw-r--r-- 1 root root 1.1K Nov  3 10:45 mel_stats.pth\n-rw-r--r-- 1 root root 1.8G Nov  3 10:45 model.pth\n-rw-r--r-- 1 root root 7.4M Nov  3 10:45 speakers_xtts.pth\n-rw-r--r-- 1 root root 353K Nov  3 10:44 vocab.json\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 10 (CORRECTED): Extend Vocabulary for Sinhala\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"EXTENDING VOCABULARY FOR SINHALA\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Verify vocab.json exists before running extend_vocab\n",
        "vocab_check_path = \"/kaggle/working/checkpoints/XTTS-v2/vocab.json\"\n",
        "if not os.path.exists(vocab_check_path):\n",
        "    print(f\"âŒ ERROR: vocab.json not found at {vocab_check_path}\")\n",
        "    print(\"âŒ Please run Cell 9 first to set up the directory structure!\")\n",
        "else:\n",
        "    print(f\"âœ… Found vocab.json at {vocab_check_path}\")\n",
        "    print(f\"\\nğŸ”¹ Running vocabulary extension...\")\n",
        "\n",
        "    # Run extend_vocab with correct path\n",
        "    !python extend_vocab_config.py \\\n",
        "        --output_path=/kaggle/working/checkpoints/ \\\n",
        "        --metadata_path=/kaggle/working/datasets/metadata_train.csv \\\n",
        "        --vocab_size=2500\n",
        "\n",
        "\n",
        "    print(\"\\nâœ… Vocabulary extension completed!\")\n",
        "\n",
        "    # Verify the extended vocab\n",
        "    vocab_path = \"/kaggle/working/checkpoints/XTTS-v2/vocab.json\"\n",
        "    if os.path.exists(vocab_path):\n",
        "        with open(vocab_path, 'r', encoding='utf-8') as f:\n",
        "            vocab = json.load(f)\n",
        "        print(f\"âœ… Extended vocabulary size: {len(vocab)} tokens\")\n",
        "\n",
        "        # Check for Sinhala characters in vocab\n",
        "        sinhala_tokens = [token for token in vocab.keys() if any('\\u0D80' <= char <= '\\u0DFF' for char in token)]\n",
        "        print(f\"âœ… Sinhala-specific tokens: {len(sinhala_tokens)}\")\n",
        "\n",
        "        # Show sample Sinhala tokens\n",
        "        if sinhala_tokens:\n",
        "            print(f\"\\nğŸ”¹ Sample Sinhala tokens: {sinhala_tokens[:10]}\")\n",
        "    else:\n",
        "        print(\"âŒ Vocabulary file not found after extension!\")\n",
        "\n",
        "print(\"=\" * 80)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-03T10:45:10.649240Z",
          "iopub.execute_input": "2025-11-03T10:45:10.649568Z",
          "iopub.status.idle": "2025-11-03T10:45:11.582071Z",
          "shell.execute_reply.started": "2025-11-03T10:45:10.649539Z",
          "shell.execute_reply": "2025-11-03T10:45:11.581047Z"
        },
        "id": "HdDeSphdVx-i",
        "outputId": "9bf6a999-ee44-4cfe-b57e-b3b310bcaa49"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "================================================================================\nEXTENDING VOCABULARY FOR SINHALA\n================================================================================\nâœ… Found vocab.json at /kaggle/working/checkpoints/XTTS-v2/vocab.json\n\nğŸ”¹ Running vocabulary extension...\nusage: extend_vocab_config.py [-h] --metadata_path METADATA_PATH\n                              [--output_path OUTPUT_PATH]\n                              [--vocab_size VOCAB_SIZE]\nextend_vocab_config.py: error: unrecognized arguments: --language=si --extended_vocab_size=2500\n\nâœ… Vocabulary extension completed!\nâœ… Extended vocabulary size: 9 tokens\nâœ… Sinhala-specific tokens: 0\n================================================================================\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zBpsXR95Vx-j"
      }
    }
  ]
}
